{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EEG analysis - part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import linear_model, svm\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import mean_squared_error, r2_score, accuracy_score, classification_report\n",
    "from sklearn.feature_selection import chi2, mutual_info_classif, f_classif\n",
    "import os\n",
    "import Queue\n",
    "import sys\n",
    "import tarfile\n",
    "import gzip\n",
    "import pandas\n",
    "import random\n",
    "from pyunicorn.timeseries.recurrence_plot import RecurrencePlot\n",
    "import nolds\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from IPython.display import display, Image\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") #, category=DeprecationWarning) \n",
    "\n",
    "# %matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset.pickle', 'rb') as f:\n",
    "    dataset = pickle.load(f)\n",
    "    \n",
    "with open('labels.pickle', 'rb') as f:\n",
    "    labels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(dataset, *splits):\n",
    "    train_dataset, train_labels = dataset[:splits[0]], labels[:splits[0]]\n",
    "    test_dataset, test_labels = dataset[splits[0]:splits[1]], labels[splits[0]:splits[1]]\n",
    "    valid_dataset, valid_labels = dataset[splits[1]:], labels[splits[1]:]\n",
    "    \n",
    "    return train_dataset, train_labels, test_dataset, test_labels, valid_dataset, valid_labels\n",
    "    \n",
    "train_dataset, train_labels, test_dataset, test_labels, valid_dataset, valid_labels = split(dataset, 1000, 1100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((array([499, 501]), array([0. , 0.5, 1. ])),\n",
       " (array([58, 42]), array([0. , 0.5, 1. ])),\n",
       " (array([43, 57]), array([0. , 0.5, 1. ])))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validate we have balanced datasets\n",
    "np.histogram(train_labels, bins=2), np.histogram(test_labels, bins=2), np.histogram(valid_labels, bins=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = to_categorical(train_labels, num_classes=2)\n",
    "test_labels = to_categorical(test_labels, num_classes=2)\n",
    "valid_labels = to_categorical(valid_labels, num_classes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 256\n",
    "num_channels = 5\n",
    "batch_size = 25\n",
    "epochs=25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), input_shape=(image_size, image_size, num_channels), batch_size=batch_size, data_format='channels_last'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(32, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "1000/1000 [==============================] - 10s 10ms/step - loss: 7.6396 - acc: 0.5010\n",
      "Epoch 2/25\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 3.6462 - acc: 0.4910\n",
      "Epoch 3/25\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.7203 - acc: 0.5510\n",
      "Epoch 4/25\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.7233 - acc: 0.5685\n",
      "Epoch 5/25\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.7406 - acc: 0.6305\n",
      "Epoch 6/25\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.6718 - acc: 0.6880\n",
      "Epoch 7/25\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.6044 - acc: 0.7360\n",
      "Epoch 8/25\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.5075 - acc: 0.7770\n",
      "Epoch 9/25\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4175 - acc: 0.8300\n",
      "Epoch 10/25\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.4268 - acc: 0.8500\n",
      "Epoch 11/25\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.2109 - acc: 0.9235\n",
      "Epoch 12/25\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.1966 - acc: 0.9445\n",
      "Epoch 13/25\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.2291 - acc: 0.9415\n",
      "Epoch 14/25\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.1303 - acc: 0.9660\n",
      "Epoch 15/25\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.0732 - acc: 0.9685\n",
      "Epoch 16/25\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.0818 - acc: 0.9750\n",
      "Epoch 17/25\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.2562 - acc: 0.9635\n",
      "Epoch 18/25\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.2238 - acc: 0.9655\n",
      "Epoch 19/25\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.1189 - acc: 0.9740\n",
      "Epoch 20/25\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.0768 - acc: 0.9855\n",
      "Epoch 21/25\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.0200 - acc: 0.9940\n",
      "Epoch 22/25\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.0742 - acc: 0.9880\n",
      "Epoch 23/25\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.2581 - acc: 0.9685\n",
      "Epoch 24/25\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.0412 - acc: 0.9895\n",
      "Epoch 25/25\n",
      "1000/1000 [==============================] - 9s 9ms/step - loss: 0.0362 - acc: 0.9890\n",
      "100/100 [==============================] - 1s 8ms/step\n",
      "[1.6091054677963257, 0.7299999445676804]\n"
     ]
    }
   ],
   "source": [
    "model = define_model()\n",
    "model.fit(\n",
    "        train_dataset, train_labels,\n",
    "        batch_size=batch_size,\n",
    "#         steps_per_epoch=2000 // batch_size,\n",
    "        epochs=epochs,\n",
    "#         validation_data=(valid_dataset, valid_labels),\n",
    "#         validation_steps=800 // batch_size\n",
    "        )\n",
    "\n",
    "print(model.evaluate(test_dataset, test_labels, batch_size=batch_size))\n",
    "# model.save_weights('.h5') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With some tweaking, we have almost 73% accuracy - not breathtaking, but not bad, either, given we expected little to no relationship in this data at all! (This is just a toy dataset - the orignal experiment was designed to expose a different relationship.)\n",
    "\n",
    "Let's try to find the best parameters for it with grid search, just for fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
