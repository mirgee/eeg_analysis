{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "\n",
    "from IPython.display import display, Image\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://commondatastorage.googleapis.com/books1000/'\n",
    "last_percent_reported = None\n",
    "data_root = '.'\n",
    "        \n",
    "def maybe_download(filename, expected_bytes, force=False):\n",
    "    dest_filename = os.path.join(data_root, filename)\n",
    "    if force or not os.path.exists(dest_filename):\n",
    "        print('Attempting to download:', filename) \n",
    "        filename, _ = urlretrieve(url + filename, dest_filename)\n",
    "        print('\\nDownload Complete!')\n",
    "    statinfo = os.stat(dest_filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified', dest_filename)\n",
    "    else:\n",
    "        raise Exception(\n",
    "          'Failed to verify ' + dest_filename + '.')\n",
    "    return dest_filename\n",
    "\n",
    "train_filename = maybe_download('notMNIST_large.tar.gz', 247336696)\n",
    "test_filename = maybe_download('notMNIST_small.tar.gz', 8458043)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "np.random.seed(133)\n",
    "\n",
    "def maybe_extract(filename, force=False):\n",
    "    root = os.path.splitext(os.path.splitext(filename)[0])[0] # remove extensions\n",
    "    if os.path.isdir(root) and not force:\n",
    "        print('%s already present - Skipping extraction of %s.' % (root, filename))\n",
    "    else:\n",
    "        print('Extracting data for %s. This may take a while. Please wait.' % root)\n",
    "        tar = tarfile.open(filename)\n",
    "        sys.stdout.flush()\n",
    "        tar.extractall(data_root)\n",
    "        tar.close()\n",
    "    data_folders = [\n",
    "        os.path.join(root, d) for d in sorted(os.listdir(root)) if os.path.isdir(os.path.join(root, d))]\n",
    "        if len(data_folders) != num_classes:\n",
    "            raise Exception(\n",
    "                'Expected %d folders, one per class. Found %d instead.' % (num_classes, len(data_folders)))\n",
    "    print(data_folders)\n",
    "    return data_folders\n",
    "  \n",
    "train_folders = maybe_extract(train_filename)\n",
    "test_folders = maybe_extract(test_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from IPython.display import Image\n",
    "\n",
    "def display_random(folder):\n",
    "    selected = random.choice([x for x in os.listdir(folder) if os.path.isfile(os.path.join(folder, x))])\n",
    "    # print(os.path.join(folder, selected))\n",
    "    display(Image(filename=os.path.join(folder, selected)))\n",
    "\n",
    "foldername = \"notMNIST_large\"\n",
    "\n",
    "for letter in os.listdir(foldername):\n",
    "    print(letter)\n",
    "    display_random(os.path.join(foldername, letter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 28  # Pixel width and height.\n",
    "pixel_depth = 255.0  # Number of levels per pixel.\n",
    "\n",
    "def load_letter(folder, min_num_images):\n",
    "    \"\"\"Load the data for a single letter label.\"\"\"\n",
    "    image_files = os.listdir(folder)\n",
    "    dataset = np.ndarray(shape=(len(image_files), image_size, image_size),\n",
    "                         dtype=np.float32)\n",
    "    print(folder)\n",
    "    num_images = 0\n",
    "    for image in image_files:\n",
    "        image_file = os.path.join(folder, image)\n",
    "        try:\n",
    "            image_data = (imageio.imread(image_file).astype(float) - \n",
    "                        pixel_depth / 2) / pixel_depth\n",
    "            if image_data.shape != (image_size, image_size):\n",
    "                raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n",
    "            dataset[num_images, :, :] = image_data\n",
    "            num_images = num_images + 1\n",
    "        except (IOError, ValueError) as e:\n",
    "            print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\n",
    "    \n",
    "    dataset = dataset[0:num_images, :, :]\n",
    "    if num_images < min_num_images:\n",
    "        raise Exception('Many fewer images than expected: %d < %d' %\n",
    "                    (num_images, min_num_images))\n",
    "    \n",
    "    print('Full dataset tensor:', dataset.shape)\n",
    "    print('Mean:', np.mean(dataset))\n",
    "    print('Standard deviation:', np.std(dataset))\n",
    "    return dataset\n",
    "        \n",
    "def maybe_pickle(data_folders, min_num_images_per_class, force=False):\n",
    "    dataset_names = []\n",
    "        for folder in data_folders:\n",
    "            set_filename = folder + '.pickle'\n",
    "            dataset_names.append(set_filename)\n",
    "    if os.path.exists(set_filename) and not force:\n",
    "        print('%s already present - Skipping pickling.' % set_filename)\n",
    "    else:\n",
    "        print('Pickling %s.' % set_filename)\n",
    "        dataset = load_letter(folder, min_num_images_per_class)\n",
    "        try:\n",
    "            with open(set_filename, 'wb') as f:\n",
    "                pickle.dump(dataset, f, pickle.HIGHEST_PROTOCOL)\n",
    "        except Exception as e:\n",
    "            print('Unable to save data to', set_filename, ':', e)\n",
    "  \n",
    "    return dataset_names\n",
    "\n",
    "train_datasets = maybe_pickle(train_folders, 45000)\n",
    "test_datasets = maybe_pickle(test_folders, 1800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_numbers_of_images(data_folders):\n",
    "    for folder in data_folders:\n",
    "        pickle_filename = folder + '.pickle'\n",
    "        try:\n",
    "            with open(pickle_filename, 'rb') as f:\n",
    "                dataset = pickle.load(f)\n",
    "        except:\n",
    "            print('Unable to read data from ', pickle_filename, \":\", e)\n",
    "        print(\"Number of images in \", folder, \": \", len(dataset))\n",
    "\n",
    "display_numbers_of_images(train_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_arrays(nb_rows, img_size):\n",
    "    if nb_rows:\n",
    "        dataset = np.ndarray((nb_rows, img_size, img_size), dtype=np.float32)\n",
    "        labels = np.ndarray(nb_rows, dtype=np.int32)\n",
    "    else:\n",
    "        dataset, labels = None, None\n",
    "    return dataset, labels\n",
    "\n",
    "def merge_datasets(pickle_files, train_size, valid_size=0):\n",
    "    num_classes = len(pickle_files)\n",
    "    valid_dataset, valid_labels = make_arrays(valid_size, image_size)\n",
    "    train_dataset, train_labels = make_arrays(train_size, image_size)\n",
    "    vsize_per_class = valid_size // num_classes\n",
    "    tsize_per_class = train_size // num_classes\n",
    "\n",
    "    start_v, start_t = 0, 0\n",
    "    end_v, end_t = vsize_per_class, tsize_per_class\n",
    "    end_l = vsize_per_class+tsize_per_class\n",
    "    for label, pickle_file in enumerate(pickle_files):       \n",
    "        try:\n",
    "            with open(pickle_file, 'rb') as f:\n",
    "                letter_set = pickle.load(f)\n",
    "                np.random.shuffle(letter_set)\n",
    "                \n",
    "                if valid_dataset is not None:\n",
    "                    valid_letter = letter_set[:vsize_per_class, :, :]\n",
    "                    valid_dataset[start_v:end_v, :, :] = valid_letter\n",
    "                    valid_labels[start_v:end_v] = label\n",
    "                    start_v += vsize_per_class\n",
    "                    end_v += vsize_per_class\n",
    "                    \n",
    "                train_letter = letter_set[vsize_per_class:end_l, :, :]\n",
    "                train_dataset[start_t:end_t, :, :] = train_letter\n",
    "                train_labels[start_t:end_t] = label\n",
    "                start_t += tsize_per_class\n",
    "                end_t += tsize_per_class\n",
    "        except Exception as e:\n",
    "            print('Unable to process data from', pickle_file, ':', e)\n",
    "            raise\n",
    "    \n",
    "    return valid_dataset, valid_labels, train_dataset, train_labels\n",
    "            \n",
    "            \n",
    "train_size = 200000\n",
    "valid_size = 10000\n",
    "test_size = 10000\n",
    "\n",
    "valid_dataset, valid_labels, train_dataset, train_labels = merge_datasets(\n",
    "  train_datasets, train_size, valid_size)\n",
    "_, _, test_dataset, test_labels = merge_datasets(test_datasets, test_size)\n",
    "\n",
    "print('Training:', train_dataset.shape, train_labels.shape)\n",
    "print('Validation:', valid_dataset.shape, valid_labels.shape)\n",
    "print('Testing:', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize(dataset, labels):\n",
    "    permutation = np.random.permutation(labels.shape[0])\n",
    "    shuffled_dataset = dataset[permutation,:,:]\n",
    "    shuffled_labels = labels[permutation]\n",
    "    return shuffled_dataset, shuffled_labels\n",
    "train_dataset, train_labels = randomize(train_dataset, train_labels)\n",
    "test_dataset, test_labels = randomize(test_dataset, test_labels)\n",
    "valid_dataset, valid_labels = randomize(valid_dataset, valid_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_labels = {0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E', 5: 'F', 6: 'G', 7: 'H', 8: 'I', 9: 'J'}\n",
    "\n",
    "def displ_random(dataset, label):\n",
    "    for i, item_num in enumerate(random.sample(range(len(label)), 8)):\n",
    "        plt.subplot(2, 4, i+1)\n",
    "        plt.axis('off')\n",
    "        plt.title(pretty_labels[label[item_num]])\n",
    "        plt.imshow(dataset[item_num])\n",
    "        \n",
    "displ_random(train_dataset, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle_file = os.path.join(data_root, 'notMNIST.pickle')\n",
    "\n",
    "try:\n",
    "    f = open(pickle_file, 'wb')\n",
    "    save = {\n",
    "        'train_dataset': train_dataset,\n",
    "        'train_labels': train_labels,\n",
    "        'valid_dataset': valid_dataset,\n",
    "        'valid_labels': valid_labels,\n",
    "        'test_dataset': test_dataset,\n",
    "        'test_labels': test_labels,\n",
    "    }\n",
    "    pickle.dump(save, f, pickle.HIGHEST_PROTOCOL)\n",
    "    f.close()\n",
    "except Exception as e:\n",
    "    print('Unable to save data to', pickle_file, ':', e)\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape(\n",
    "        (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Pad images with 0s\n",
    "train_dataset     = np.pad(train_dataset, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "valid_dataset = np.pad(valid_dataset, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "test_dataset       = np.pad(test_dataset, ((0,0),(2,2),(2,2),(0,0)), 'constant')\n",
    "    \n",
    "print(\"Updated Image Shape: {}\".format(train_dataset[0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "index = random.randint(0, len(train_dataset))\n",
    "image = train_dataset[index].squeeze()\n",
    "\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.imshow(image, cmap=\"gray\")\n",
    "print(np.argmax(train_labels[index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check classes in validation set are balanced\n",
    "np.count_nonzero(valid_labels, axis=0)\n",
    "valid_labelscc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 28 + 4\n",
    "batch_size = 16\n",
    "beta = 1e-3\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    \n",
    "    # Variables\n",
    "    c1_w = tf.Variable(tf.truncated_normal([5, 5, num_channels, 6], stddev=0.1))\n",
    "    c1_b = tf.Variable(tf.zeros([6]))\n",
    "    \n",
    "    c3_w = tf.Variable(tf.truncated_normal([5, 5, 6, 16], stddev=0.1))\n",
    "    c3_b = tf.Variable(tf.zeros(16))\n",
    "    \n",
    "    f5_w = tf.Variable(tf.truncated_normal([400, 120], stddev=0.1))\n",
    "    f5_b = tf.Variable(tf.zeros(120))\n",
    "    \n",
    "    f6_w = tf.Variable(tf.truncated_normal([120, 84], stddev=0.1))\n",
    "    f6_b = tf.Variable(tf.zeros(84))\n",
    "    \n",
    "    f7_w = tf.Variable(tf.truncated_normal([84, 10], stddev=0.1))\n",
    "    f7_b = tf.Variable(tf.zeros(10))\n",
    "    def model(data):\n",
    "        # C1: 32x32x1 -> 28x28x6\n",
    "        \n",
    "        c1 = tf.nn.conv2d(data, c1_w, [1, 1, 1, 1], padding='VALID') + c1_b\n",
    "        c1 = tf.nn.relu(c1)\n",
    "        shape = c1.get_shape().as_list()\n",
    "        assert(shape[1] == 28 and shape[2] == 28 and shape[3] == 6), \"The shape is %s\" % shape\n",
    " \n",
    "        # S2: 28x28x6 -> 14x14x6\n",
    "        s2 = tf.nn.max_pool(c1, [1, 2, 2, 1], [1, 2, 2, 1], padding='VALID')\n",
    "        shape = s2.get_shape().as_list()\n",
    "        assert(shape[1] == 14 and shape[2] == 14 and shape[3] == 6), \"The shape is %s\" % shape\n",
    "        \n",
    "        # C3: 14x14x6 -> 10x10x16\n",
    "        c3 = tf.nn.conv2d(s2, c3_w, [1,1,1,1], padding=\"VALID\") + c3_b\n",
    "        c3 = tf.nn.relu(c3)\n",
    "        shape = c3.get_shape().as_list()\n",
    "        assert(shape[1] == 10 and shape[2] == 10 and shape[3] == 16), \"The shape is %s\" % shape\n",
    "\n",
    "        # S4: 10x10x16 -> 5x5x16\n",
    "        s4 = tf.nn.max_pool(c3, [1,2,2,1], [1,2,2,1], padding='VALID')\n",
    "        shape = s4.get_shape().as_list()\n",
    "        assert(shape[1] == 5 and shape[2] == 5 and shape[3] == 16), \"The shape is %s\" % shape\n",
    "\n",
    "        # F5: 5x5x16 -> 120\n",
    "        f5_in = tf.contrib.layers.flatten(s4)\n",
    "#         shape = s4.get_shape().as_list()\n",
    "#         f5_in = tf.reshape(s4, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        f5 = tf.nn.relu(tf.matmul(f5_in, f5_w) + f5_b)\n",
    "\n",
    "        # F6: 120 -> 84\n",
    "        f6 = tf.nn.relu(tf.matmul(f5, f6_w) + f6_b)\n",
    "\n",
    "        # F7: 84 -> 10\n",
    "        f7 = tf.matmul(f6, f7_w) + f7_b # ???\n",
    "        return f7\n",
    "    \n",
    "    # Training computation.\n",
    "    logits = model(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) + \\\n",
    "            beta*(tf.nn.l2_loss(c1_w) + tf.nn.l2_loss(c3_w) + tf.nn.l2_loss(f5_w) + tf.nn.l2_loss(f6_w) + tf.nn.l2_loss(f7_w))\n",
    "    \n",
    "\n",
    "    # Optimizer.\n",
    "    learning_rate = tf.train.inverse_time_decay(0.05, global_step, 500, 0.85, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 10001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 50 == 0):\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(\n",
    "    valid_prediction.eval(), valid_labels))\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
